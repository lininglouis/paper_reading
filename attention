attention能够学习复杂的相互映射关系，但是对于顺序无感。需要额外操作。

attention模型在预测每个输出(一共m个输出元素），都需要查一遍所有的source input序列元素(n)，然后根据attention权重，计算得到context。
这就意味着，每次计算输出元素，都需要访问一遍n个输入，就像查表操作。而不像attention。

With an attention mechanism we no longer try encode the full source sentence into a fixed-length vector. **Rather, we allow the decoder to “attend” to different parts of the source sentence at each step of the output generation**
Actually, that’s quite counterintuitive. Human attention is something that’s supposed to save computational resources. By focusing on one thing, we can neglect many other things. But that’s not really what we’re doing in the above model. We’re essentially looking at everything in detail before deciding what to focus on. Intuitively that’s equivalent outputting a translated word, and then going back through all of your internal memory of the text in order to decide which word to produce next. That seems like a waste, and not at all what humans are doing. In fact, it’s more akin to memory access, not attention, which in my opinion is somewhat of a misnomer (more on that below). Still, that hasn’t stopped attention mechanisms from becoming quite popular and performing well on many tasks.
An alternative approach to attention is to use Reinforcement Learning to predict an approximate location to focus to. That sounds a lot more like human attention, and that’s what’s done in Recurrent Models of Visual Attention.



可视化
相比传统rnn更加灵活，适合复杂多模态的映射




在翻译任务中，Query可以视为原语词向量序列，而Key和Value可以视为目标语词向量序列。一般的注意力机制可解释为计算Query和Key之间的相似性，并利用这种相似性确定Query和Value之间的注意力关系。


首先在编码器到解码器的层级中，Query来源于前面解码器的输出，而记忆的Key与Value都来自编码器的输出。这允许解码器中的每一个位置都注意输入序列中的所有位置，因此它实际上模仿了序列到序列模型中典型的编码器-解码器注意力机制。
 


attention structure
https://zhuanlan.zhihu.com/p/31547842

luong attention and xx attention 
http://cnyah.com/2017/08/01/attention-variants/


