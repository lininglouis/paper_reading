attention能够学习复杂的相互映射关系，但是对于顺序无感。需要额外操作。

attention模型在预测每个输出(一共m个输出元素），都需要查一遍所有的source input序列元素(n)，然后根据attention权重，计算得到context。
这就意味着，每次计算输出元素，都需要访问一遍n个输入，就像查表操作。而不像attention。

With an attention mechanism we no longer try encode the full source sentence into a fixed-length vector. **Rather, we allow the decoder to “attend” to different parts of the source sentence at each step of the output generation**
Actually, that’s quite counterintuitive. Human attention is something that’s supposed to save computational resources. By focusing on one thing, we can neglect many other things. But that’s not really what we’re doing in the above model. We’re essentially looking at everything in detail before deciding what to focus on. Intuitively that’s equivalent outputting a translated word, and then going back through all of your internal memory of the text in order to decide which word to produce next. That seems like a waste, and not at all what humans are doing. In fact, it’s more akin to memory access, not attention, which in my opinion is somewhat of a misnomer (more on that below). Still, that hasn’t stopped attention mechanisms from becoming quite popular and performing well on many tasks.
An alternative approach to attention is to use Reinforcement Learning to predict an approximate location to focus to. That sounds a lot more like human attention, and that’s what’s done in Recurrent Models of Visual Attention.



可视化
相比传统rnn更加灵活，适合复杂多模态的映射


